---
layout: post
title: PyTorch로 시작하는 딥러닝 입문 (4)-1 선형회귀
categories: [PyTorch]
---

---

이번 포스팅은 [PyTorch로 시작하는 딥러닝 입문](https://wikidocs.net/53560) 선형회귀 챕터이다.<br>
이 챕터에서는 선형회귀 이론에 대해 이해하고, PyTorch를 이용해서 선형회귀 모델을 만들어 볼 것이다.<br>
모델을 만들기 전에 알아야 할 기본적인 내용에 대해서도 정리하고 갈 것이다. 내용은 다음과 같다. <br>

---

+ __데이터에 대한 이해__
  + 학습과 추론에 쓰이는 데이터에 대해 알아본다. <br><br>
+ __가설 수립__
  + 가설을 수립 방법에 대해 알아본다. <br><br>
+ __손실 계산__
  + 학습 데이터로 모델을 학습할 때 생기는 오차와 그 오차를 계산하는 방법을 알아본다. <br><br>
+ __경사 하강법__
  + 학습 시 가장 중요하다 볼 수 있는 최적화 방법 중 경사 하강법을 알아본다. <br><br><br>


## __1. 데이터에 대한 이해__
---

우선 딥러닝 모델에 쓰이는 데이터에 대해 알아 볼 것이다. <br>
딥러닝에는 많은 양의 데이터셋이 사용된다. 보통 사용되는 데이터셋은 학습 __(training)__, 검증 __(validation)__, 테스트 __(test)__ <br>
데이터셋으로 나뉜다. 이 셋은 원래의 데이터셋을 특정한 비율라 나누어서 사용하는데, 보통은 학습에 사용되는 데이터가 <br>
가장 많다. 학습 데이터는 초기화된 모델의 파라미터들을 데이터를 가장 잘 추론하는 방향으로 업데이트할 때 사용된다. <br>
검증 데이터는 학습 데이터의 일부를 떼서 사용하는데, 이 데이터는 학습된 모델의 성능을 바로 테스트 하기 전에 <br>
모델의 성능을 점검한다. 많은 사람들이 모의 고사에 비유하고는 하는데, 그에 맞게 모델은 검증 데이터를 이용해 <br>
하이퍼 파라미터를 조정한다. 그로 인해 모델의 성능을 조금 더 끌어 올릴 수 있게 된다. <br>
훈련 데이터셋에서 검증 데이터를 나누는 방법에는 여러가지가 있다. 이는 나중에 따로 정리하던가 하려 한다.<br>
이렇게 검증 단계까지 거친 후 모델은 최종적으로 테스트 데이터셋으로 모델의 성능을 평가하게 된다. <br><br>

딥러닝 모델에 입력이 주어지면 모델은 연산을 통해 최적의 추론값을을 출력한다. 그래서 모델을 훈련할 때 사용되는 <br>
데이터셋은 입력값과 출력값(정답)이 짝지어져 있으며, 입력-출력 짝이 여러개 존재한다. 이 때 일반적으로 입력을 x <br>
출력을 y라고 한다. 이 출력 y를 정답값, 라벨 등으로 부르기도 하며, 나는 보통 라벨로 부르고 있다.<br><br><br><br>


## __2. 가설 수립__
---

머신러닝, 딥러닝에서 식을 세울 때 그 식을 가설 __(Hypothesis)__ 이라고 부른다. 이 가설은 임의로 추측해서 세운 식이거나 <br>
이미 경험적으로 알고있는 식일 수 도 있다. 그리고 가설이 틀렸다면 수정해나가기도 한다.<br><br>

우선 가설에 대해 말하기 전에 이 챕터의 제목이기도한 선형 회귀 __(Linear Regression)__ 에 대해 알아보자. <br>
선형 회귀는 데이터가 분포해 있을 때 그 데이터를 놓고 가장 잘 설명할 수 있는 선을 찾는 분석 방법을 말한다<br><br>

이 선형 회귀의 가설은 설명했듯이 직선의 형태를 띄게 된다. 그래서 선형 회귀의 가설 식은 직선의 방정식이 된다. <br>

$$ y = Wx + b $$

$$ H(x) = Wx + b $$

위와 같이 선형 회귀의 가설식을 표현할 수 있다. 여기서 결과값을 $y$ 로 표현할 수도 있고 $H(x)$로도 할 수도 있다. <br>
여기서 $W$를 가중치 __(Weight)__, $b$를 편향 __(bias)__ 이라고 부른다. <br><br><br>


## __3. 손실 계산__
---

딥러닝을 공부하다보면 학습 시의 오차에 관련해 다음과 같은 말들을 자주 접하게 되는데, 전부 같은 단어라 보면 된다.<br>
__비용함수(Cost function) = 손실 함수(Loss function) = 오차 함수(Error function) = 목적 함수(Objective function)__ <br>
나는 이 중에서 주로 손실 함수나 목적 함수라는 말을 쓴다. 결국 의미는 같으므로 큰 상관은 없다. <br><br>

손실 함수는 말 그대로 딥러닝 모델의 가설을 통해 도출한 결과값과 실제 데이터의 정답값 간의 오차를 나타내는 함수이다. <br>
하지만 정답값과 예측값의 차이만을 이용해 오차를 정의하면 정답값 - 예측값의 계산결과가 음수가 나올 수도 있다. <br>
그렇게 된다면 실제로는 오차가 매우 큼에도 불구하고 오차가 거의없게 될 수도 있어 별도의 오차 계산 과정이 필요하다. <br>
그래서 만들어진 것이 손실 함수이며, 손실 함수의 종류는 여러가지가 있다. __Cross Entropy, Mean Square Error,__ <br>
__Root Mean Square Error__ 등 여러 종류가 있지만, 우선 여기서는 __Mean Square Error (MSE)__ 만 이야기 할 것이다. <br>
우선 MSE는 평균 제곱 오차라는 뜻으로, 측정값과 정답값의 차이 만으로 오차를 내지 않고 차이의 제곱 합을 평균내어 오차로 <br>
사용한다. 수식은 다음과 같다. <br>

$$ Cost(W,b) = {1 \over n} \sum_{i=1}^n [y^{(i)} - H(x^{(i)})]^2 $$

위는 MSE 수식을 나타낸 것이며, 이 때 파라미터는 W와 b 두 가지 이다. 회귀 문제에 가장 적절한 비용 함수는 MSE 이고, <br>
위 식의 결과값을 최소화하는 방향으로 W와 b값을 업데이트한다. <br><br><br>


## __4. 경사하강법 (Optimizer)__
---
손실 함수의 값을 최소화하는 W와 b를 찾기 위해서는 optimizer가 필요하다. 이 떄 optimizer를 이용해서 훈련데이터를 가장 <br>
잘 표현하는 W와 b를 찾아가는 과정을 학습(training)이라고 부른다. 여기서 가장 기본적이고 간단한 방법이 경사하강법, <br>
__Gradient Descent__ 이다.

<p align="center">
  <img src="/assets/img/PyTorch_wikidocs/optimizer_Wandcost_img.png" >
</p>

우선 편의를 위해 가설을 $$ H(x) = Wx $$ 로 가정하고 경사하강법에 대해 정리하겠다. 위 그래프에서 W의 크기가 무한정 <br>
커지게 되면, 손실 함수의 값이 무한대로 커지게 될 것이다. 그렇다고 W를 너무 작게 설정해도 마찬가지로 무한대로 발산하게 <br>
될 것이다. 손실 함수 결과값을 최소화하는 W값을 갖기 위해서는 위 그래프에서 맨 아래의 볼록한 위치로 가야될 것이다. <br>
<p align="center">
  <img src="/assets/img/PyTorch_wikidocs/gradient_descent_img.png" >
</p>
위의 그림과 같이 경사를 따라 맨 아래쪽의 볼록한 위치로 이동하도록 W값을 수정해 나가는 것을 경사 하강법 (Gradient Descent) <br>
라고 한다. 그래프에서 기울기가 무한정 커지거나 작아지게 되면 손실함수의 값은 무한정 커지게 된다. 그래서 최적의 W값을 찾기 <br>
위해서는 접선의 기울기가 작아지는 지점을 향해서 업데이트해야 된다. 이 때 기울기가 양수라면 W값을 감소시키고, 기울기가 <br>
음수라면 W값을 증가시키면 된다. 

$$ 기울기 = {\partial cost(W) \over \partial W} $$

W값을 조정할 때에는 현재 W에서 구한 기울기에 특정값 $$ \alpha $$ 를 곱한 값을 빼서 새로운 W를 만든다. 정리하자면 <br>

$$ W = W - \alpha {\partial \over \partial W} cost(W) $$

의 식이 경사하강법의 W 업데이트 식이 된다. <br><br>
여기서 $$ \alpha $$ 를 학습률(learning rate)이라고 부르며, 이 값은 hyperparameter로 사용자가 임의로 설정하는 값이다. <br>
학습률은 한번 W값을 변경할 때 얼마나 변경할지를 나타내며, W-Cost 그래프에서 경사를 따라 얼마나 큰 폭으로 이동할지를 <br>
결정한다. 학습률을 무작정 크게 한다면 적절한 W값을 찾지 못하고 학습속도가 느려지거나 발산하여 W의 최적값을 찾지 못하게 <br>
될 수도 있다. 그래서 적당한 크기의 학습률을 설정하는 것도 중요하다. 