---
layout: post
title: PyTorch로 시작하는 딥러닝 입문 (5) 다중 선형 회귀
categories: [PyTorch]
---

---

이전 포스팅에서 다루었던 선형 회귀는 입력 $$x$$가 하나인 단순 선형 회귀(Simple Linear Regression)라고 한다. 이번 포스팅에서는 <br>
여러개의 $$x$$가 입력되고, 입력된 다수의 $$x$$로부터 $$y$$를 예측하는 다중 선형 회귀(Multivariable Linear Regression)을 다룰 것이다. <br><br><br>


### 다중 선형 회귀의 데이터
---
다중 선형 회귀 문제에서는 독립 변수 $$x$$가 한개가 아니라 여러개 존재한다. 그에 맞춰서 수식또한 단순 선형 회귀와 달라지는데, <br>
예시로 독립 변수가 3개인 다중 선형 회귀 문제의 수식을 써보면 다음과 같다. <br>

$$ H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b $$

편향 $$b$$는 단순 선형 회귀와 같으나 $$w_1, w_2, w_3$$과 같이 가중치 파라미터가 세 개가 생겼다. 그로 인해 훈련 데이터와 가중치<br>
파라미터 선언 시에도 3 개를 선언해 주어야 한다. <br>

PyTorch로 구현하며 알아보자.<br>

```python
# 필요 module import
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Random seed 고정
torch.manual_seed(1)
```
훈련 데이터 선언 시에는 다중 선형 회귀 문제이므로 $$x$$를 여러개 선언해주어야 한다. <br>

```python
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [182], [180], [196], [142]])
```

이번엔 가중치 $$W$$와 편향 $$b$$를 선언하는데 가중치 $$w$$도 $$x$$와 마찬가지로 3개 선언한다 <br>

```python
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

그 다음은 단순 선형 회귀와 같이 optimizer 선언 후 경사하강법을 수행하면 된다 <br>

```python
# Initialize optimizer
optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)

num_epochs = 1000
for epoch in range(num_epochs+1):

    # Compute H(x)
    hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3 + b

    # Compute loss
    loss = torch.mean((y_train - hypothesis)**2)
    
    # Backpropagate and Update parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print log every 100th epoch
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))
```

<br><br><br>

위의 코드는 개선할 수가 있다. 위 코드에서는 x_train과 w를 각각 3개씩 선언했으나, 실제로 딥러닝 모델에서는 이보다 훨씬 더 많은<br>
x와 w의 수를 사용한다. 그래서 실제로 하나씩 다 선언해준다는 것은 사실상 불가능한 일이다. 그래서 이를 한번에 처리함으로써 <br>
코드를 한층 개선시켜 사용할 수 있다. <br>

### 행렬 연산으로 바꾸기
---

앞서 말했듯이 각 훈련데이터마다 연산을 모조리 다 선언해주는 것은 불가능에 가까운 일이므로 우리는 x_train들과 w들을 모아 <br>
행렬을 만들고 그를 이용해서 연산을 좀 더 쉽게 표현할 것이다. 이 때 행렬간 곱셈 과정에서 이루어지는 벡터 연산을 벡터의 내적<br>
(Dot Product)이라고 한다. 
<p align="center">
  <img src="/assets/img/PyTorch_wikidocs/matmul_img.png">
</p>

위 그림은 행렬의 연산을 나타낸 것이다. 위처럼 벡터의 내적을 통해 결과를 얻어내는데 이 연산을 가설과 연관지어 본다면,

$$  H(x) = w_1x_1 + w_2x_2 + w_3x_3 $$

위와 같은 식이 있을 때 이 식은 벡터의 내적처럼 표현할 수 있게 된다. 


$$ \begin{pmatrix} x_1 & x_2 & x_3 \end{pmatrix} \centerdot \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} = (w_1x_1 + w_2x_2 + w_3x_3) $$

이 식의 벡터 2개를 각각 $$X$$와 $$W$$로 표현한다면 가설식은 다음과 같이 나타낼 수 있다.

$$ H(X) = XW $$

이제 훈련 데이터를 적용시켜 행렬의 연산으로 표현해 보자. 훈련 데이터는 앞서 사용했던 것과 같은 것을 사용할 것이다. 
<p align="center">
  <img src="/assets/img/PyTorch_wikidocs/Mult_linearreg_data_img.png">
</p>

이 데이터는 샘플 수가 5개, 각 샘플별 특성(feature)수가 3개인 데이터이다. 여기서 각 특성들을 독립 변수 $$x$$라고 한다. <br>
이 데이터를 행렬 $$X$$로 나타내보자면, 

$$ \begin{pmatrix} x_{11} && x_{12} && x_{13} \\ x_{21} && x_{22} && x_{23} \\ x_{31} && x_{32} && x_{33} \\ x_{41} && x_{42} && x_{43} \\ x_{51} && x_{52} && x_{53} \end{pmatrix} $$

처럼 나타낼 수 있다. 여기에 가중치 $$w_1, w_2, w_3$$을 원소로 갖는 벡터 $$W$$를 곱하고 각 샘플별로 편향을 추가하면

$$ \begin{pmatrix} x_{11} && x_{12} && x_{13} \\ x_{21} && x_{22} && x_{23} \\ x_{31} && x_{32} && x_{33} \\ x_{41} && x_{42} && x_{43} \\ x_{51} && x_{52} && x_{53} \end{pmatrix} \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} + \begin{pmatrix} b \\ b \\ b \\ b \\ b \end{pmatrix} = \begin{pmatrix} x_{11}w_1 + x_{12}w_2 + x_{13}w_3 + b \\ x_{21}w_1 + x_{22}w_2 + x_{23}w_3 + b \\ x_{31}w_1 + x_{32}w_2 + x_{33}w_3 + b \\ x_{41}w_1 + x_{42}w_2 + x_{43}w_3 + b \\ x_{51}w_1 + x_{52}w_2 + x_{53}w_3 + b \end{pmatrix} $$

가 된다. 이 식을 가설로 표현하면 다음과 같다. 

$$ H(X) = XW + B $$

이렇게 행렬 연산을 하게 되면 식의 간소화 뿐만 아니라 다수 샘플의 병렬 연산을 가능하게 함으로 계산 속도 또한 빨라진다. 이제 <br>
이를 참고하여 PyTorch로 구현해 볼 것이다.<br><br>

### 행렬 연산 PyTorch로 구현하기
---
앞서 말한 다중 선형 회귀를 행렬 연산을 이용해서 구현하려면 훈련 데이터를 행렬로 선언해야 한다. <br>

```python
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 80],
                             [96, 98, 100],
                             [73, 66, 70]])  # Size : [5,3]

y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])  # Size : [5,1]
```

여러개의 x_train을 선언할 필요없이 x_train 하나로 모든 샘플, 행렬 $$X$$를 만들면 된다. 다음으로 가중치 $$W$$와 편향 $$b$$를 선언한다. <br>

```python
W = torch.zeros((3,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

가중치 행렬의 크기는 3x1이 된다. 이는 행렬의 곱연산 시 좌측 행렬 열 크기와 우측 행렬 행 크기가 일치해야 하기 때문이다. 가설의 곰셈에서는 <br>
$$X$$와 $$W$$를 곱하기에 [5,3] 크기인 $$X$$의 열 크기 3에 맞게 $$W$$의 행 크기를 3으로 맞춰주었다. 이를 토대로 가설을 선언한다면, <br>

```python
hypothesis = x_train.matmul(W) + b
```

가 된다. 이전에 x_train과 W의 곱셈 항을 모두 선언하는 것과 달리 독립 변수 $$x$$의 수를 변경하더라도 가설 코드를 수정할 필요가 없어진다. <br>
전체 코드를 정리해보면 다음과 같다. <br>

```python
x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 80],
                             [96, 98, 100],
                             [73, 66, 70]])

y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

# Initialize Model
W = torch.zeros((3,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# Set Optimizer
optimizer = optim.SGD([W, b], lr = 1e-5)

num_epochs = 20
for epoch in range(num_epochs + 1):

    # Compute Hypothesis
    hypothesis = x_train.matmul(W) + b  # bias는 브로드캐스팅돼서 각 샘플의 행렬곱 값에 더해짐

    # Compute loss
    loss = torch.mean((y_train - hypothesis)**2)

    # Update H(x)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(
        epoch, num_epochs, hypothesis.squeeze().detach(), loss.item()))
```

<br><br><br>

### nn.Module로 구현
---
이제껏 구현한 코드를 보면 nn.Module이라는 PyTorch의 모듈을 사용하지 않고 손실 함수, 가설 등을 직접 구현했다. 이번에는 <br>
PyTorch에 이미 구현되어 제공되는 함수를 이용해 좀 더 쉽게 단순 선형 회귀와 다중 선형 회귀를 구현해 보려고 한다. 우선 선형<br>
회귀 모델은 PyTorch에서 ```nn.Linear()```로, MSE Loss는 ```nn.functional.mse_loss()```로 구현되어 있다. 간단하게 예시를 보면,<br>

```python
# Linear Regression
import torch.nn as nn
model = nn.Linear(input_dim, output_dim)

# Loss function
import torch.nn.functional as F
loss = F.mse_loss(prediction, y_train)
```

위와 같이 간단히 사용할 수 있다. <br><br>


### Simple Linear Regression 구현
---
단순 선형 회귀 구현을 위해 모듈을 임포트하고 데이터를 선언하는 단계까지는 이전과 같다. 

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

# Data
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])


# Implement Linear Regression
model = nn.Linear(input_dim=1, output_dim=1)  # 단순 선형 회귀이므로 input, output 둘다 1, 1

# Initialize Optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)

num_epochs = 2000
for epoch in range(num_epochs+1):

    # Compute H(x)
    prediction = model(x_train)

    # Compute Loss
    F.mse_loss(prediction, y_train)

    # Update H(x)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, num_epochs, loss.item()))
```

위 코드에서 달라진 부분을 찾는다면, torch.nn을 사용해서 선형 회귀 모델을 만드는 부분, torch.nn.functional을 이용해서 <br>
MSE Loss를 만드는 부분, 그리고 optimizer에 전달되는 인자인 model.parameters()가 있다. <br><br>

__torch.nn.Linear()__ <br>
PyTorch 공식문서에 따르면 Linear 함수에 전달되는 인자는 다음과 같다.
```python
torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
```
그리고 설명을 보면 입력되는 데이터에 대해서 선형 변환을 수행한다고 한다. $$ y=xA^T + b $$ <br>
in_features는 입력 샘플의 크기, out_features는 출력될 샘플의 크기를 말하며 bias가 False로 설정되어 있을 경우 편향이 없는 선형 <br>
회귀가 된다.<br><br>

__torch.nn.functional.mse_loss()__<br>
torch.nn.functional.mse_loss 함수는 PyTorch에서 제공하는 MSE Loss 함수이다. <br><br>

__model.paramters()__
모델에 사용되는 파라미터들의 모음이다. 위의 단순 선형 회귀 모델에서는 가중치 $$W$$와  편향 $$b$$가 저장되어있다. 출력해보면 <br>
```python
print(list(model.parameters()))
```
```python
[Parameter containing:
tensor([[0.5153]], requires_grad=True), Parameter containing:
tensor([-0.4414], requires_grad=True)]
```

와 같이 모델의 파라미터가 출력된다. 여기서 첫번째 값이 $$W$$, 두번째 값이 $$b$$이다. 둘은 현재 랜덤하게 초기화되어 있으며 <br>
(Random Initialization), 둘 다 학습 대상이므로 requires_grad가 True로 설정되어 있다.<br>
다중 선형 회귀 모델 구현은 단순 선형 회귀 모델과 유사하며, 입력과 파라미터 값 등 세부 설정값만 다르다. <br>
하지만 아직도 실제 PyTorch로 구현한 모델이라 하기엔 조금 부족하다. <br><br>

### Class로 PyTorch 모델 구현하기
---
PyTorch로 구현한 대부분의 구현체들은 대부분 모델을 생성할 때 class를 사용해 구현한다. 위와 동일하게 선형 회귀 모델을 class로 <br>
구현해 보자. <br><br>

위에서는 단순 선형 회귀 모델을 nn.Module을 이용해서 구현했다. 
```python
model = nn.Linear(1,1)
```
이를 class를 이용해서 구현한다면 다음과 같이 만들 수 있다.
```python
class LinearRegressionModel(nn.Module):  # Inherit nn.Module
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)

# Model 생성
model = LinearRegressionModel()
```

이 클래스를 간단하게 살펴보자면 <br>
+ ```__init__()``` : 모델의 구조, 동작 정의하는 생성자, 객체 속성값 초기화 하며 객체 생성 시 자동 호출
+ super() : 생성자에 불러지면 상속받는 class의 속성을 가진채 초기화된다. 여기서는 nn.Module 상속
+ forward() : 모델이 데이터 입력받아 forward 연산 진행하도록 하는 함수. 모델을 데이터와 함께 호출 시 자동으로 실행 <br><br>

다중 선형 회귀의 경우에도 클래스 구현은 비슷하게 한다. 이제 클래스를 이용해 코드를 구현해 보면 다음과 같다. 

```python
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

torch.manual_seed(1)

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)


model = LinearRegressionModel()
optimizer = optim.SGD(model.parameters(), lr=0.01)

num_epochs = 2000
for epoch in range(num_epochs+1):
    prediction = model(x_train)

    loss = F.mse_loss(prediction, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(
          epoch, nb_epochs, cost.item()))
```
<br><br>

### Mini Batch & Data Load
---
이번에는 데이터에 관해 살펴보려한다. 지금까지 예시로 구현했던 모델들의 데이터는 샘플 수가 그렇게 많지가 않다. 하지만<br> 
현업에서의 데이터는 이보다 훨씬 방대하다. 수십, 수백만 개의 데이터를 위의 행렬 연산과 같은 방식으로 데이터 전체에 대해 <br>
경사하강법을 수행하고 모델을 학습시킨다면 계산량이 급격히 많아지며 속도 또한 떨어지게 될 것이다. 심지어는 메모리 부족으로 <br>
실행조차 안될 수도 있다. <br>
그래서 전체 데이터를 바로 사용하는 것이 아닌, 데이터를 작은 단위로 쪼개서 학습에 사용하는 방식이 고안되었다. 이를 미니 배치<br>
(Mini Batch)라고 한다. 미니 배치의 크기는 일반적으로 2의 제곱수를 사용한다. 이유는 CPU와 GPU의 메모리가 2의 배수라서 배치의 <br>
크기가 2의 제곱수일 때 데이터 송수신의 효율을 높일 수 있어서 라고 한다.

<p align="center">
  <img src="/assets/img/PyTorch_wikidocs/mini_batch_img.png">
</p>

위의 그림은 전체 데이터를 미니 배치 단위로 나누어 주는 것을 표현한다. 학습할 때에는 미니 배치 단위로  손실을 계산하고, 각 미니 <br>
배치에 대해 경사하강법을 수행해준다. 전체 미니 배치에 대해서 이러한 학습과정이 완료되면 1 에포크가 끝나게 된다. 이렇게 미니 <br>
배치 단위로 수행하는 경사하강법을 미니 매치 경사하강법 이라고 부른다. 미니 배치 경사하강법은 계산량이 비교적 적고, 훈련 <br>
속도가 빠르다는 장점이 있지만 전체 데이터를 보고 경사하강법을 수행하는 것이 아닌 데이터의 일부만을 보고 수행하기에 수렴 <br>
과정이 조금 느려질 수 있다. <br><br>

### Iteration
---
학습 횟수에 대해 설명할 때 에포크(Epoch)와 함께 이터레이션(Iteration) 또한 자주 쓰이는 말이다. 이터레이션은 한 에포크내에서의 <br>
파라미터 업데이트 횟수이다. 만약 전체 데이터 크기(배치 크기)가 2,000이라 가정할 떄, 미니 배치의 크기가 200이면, 파라미터는 총 <br>
10번 업데이트 된다. 이 때 이터레이션의 값은 10이 된다. <br><br>

### Dataset & DataLoader
---
PyTorch에서는 데이터를 다루는데 도움을 주는 도구로 Dataset과 DataLoader를 제공한다. 이 도구들로 데이터 셔플과 병렬 처리 또한 <br>
쉽게 할 수 있다. Dataset을 정의한 다음 정의한 것을 DataLoader에 전달하면 된다. 예시를 통해 사용법을 보자.<br>

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import TensorDataset  # Import TensorDataset
from torch.utils.data import DataLoader     # Import DataLoader

# Define data in Tensor shape
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  90], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

# x_train, y_train을 Tensordataset의 입력으로 하여 dataset으로 지정
dataset = TensorDataset(x_train, y_train)

dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

model = nn.Linear(3,1)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)

num_epochs = 20
for epoch in range(num_epochs + 1):
    for batch_idx, samples in enumerate(dataloader):
        x_train, y_train = samples

        # Compute H(x)
        prediction = model(x_train)

        # Compute Loss
        loss = F.mse_loss(prediction, y_train)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print('Epoch {:4d}/{} Batch {}/{} Loss: {:.6f}'.format(
        epoch, num_epochs, batch_idx+1, len(dataloader),
        loss.item()
        ))
```

```FloatTensor```로 선언한 x_train과 y_train으로 데이터셋을 만들어 데이터로더로 사용한다. 데이터로더에는 보통 2개의 인자가 전달<br>
되는데, 데이터셋과 미니 배치의 크기를 전달받는다. 또한 추가적으로 자주 사용되는 인자로는 boolean 변수인 shuffle이 있는데, 이를 <br>
True로 설정할 시 매 epoch마다 데이터셋을 섞어 핛브 순서를 바꾼다. shuffle=True로 설정했을 때 학습에 도움을 준다고 한다. <br>
이렇게 Dataset을 정의하고 DataLoader에 전달하여 데이터셋을 사용하는 방법도 있지만, ```torch.utils.data.Dataset```을 상속받아서 직접 <br>
커스텀 데이터셋(Custom Dataset)을 만드는 방법도 있다. 커스텀 데이터셋을 만드는 기본적 뼈대는 다음과 같다. <br>

```python
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self):
        # Dataset 전처리
    
    def __len__(self):
        # Dataset 길이(총 샘플 수)
        # Dataset의 크기 return

    def __getitem__(self, idx):
        # Dataset에서 특정 샘플 1개 가져오기
        # dataset[i]로 i번째 샘플 가져오게 함
```

선형 회귀 모델에 이 커스텀 데이터셋을 구현하려면 다음과 같이 하면 된다. <br>

```python
import torch
import torch.nn.functional as F

from torch.utils.data import Dataset
from torch.utils.data import DataLoader


# torch.utils.data.Dataset 상속
class CustormDataset(Dataset):
    def __init__(self):
        self.x_data = [[73, 80, 75],
                   [93, 88, 93],
                   [89, 91, 90],
                   [96, 98, 100],
                   [73, 66, 70]]
        self.y_data = [[152], [185], [180], [196], [142]]

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        x = torch.FloatTensor(self.x_data[idx])
        y = torch.FloatTensor(self.y_data[idx])
        return x, y

dataset = CustomDataset()
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
```

이후의 부분은 동일하게 진행하면 된다. <br><br>

---

여기까지 선형 회귀 파트에서 단순 선형 회귀 모델과 다중 선형 회귀 모델에 대해서 다루었다. 동시에 PyTorch에서 제공하는 유용한 <br>
기능인 Autograd, nn.Module, Dataset, DataLoader, 그리고 PyTorch를 이용한 모델 설계 시 기본이 되는 클래스를 이용하여 간단한 <br>
모델도 만들어 보았다. Optimizer, SGD(Stochastic Gradient Descent)와 같이 모델의 학습에 사용되는 기능들도 알아보았다. <br>
이번 챕터에서 중요하다 생각되는 키워드들은
+ Simple/Multivariable Linear Regression
+ nn.Module
+ Autograd
+ Dataset & DataLoader
+ optimizer
+ Stochastic Gradient Descent
<br>
정도가 될 것 같다.